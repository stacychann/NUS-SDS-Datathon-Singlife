{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "%pip install pyarrow\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read the Parquet file into a PyArrow Table\n",
    "table = pq.read_table(filepath)\n",
    "\n",
    "# Convert the PyArrow Table to a Pandas DataFrame if needed\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Now you can work with the DataFrame or the PyArrow Table as needed\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection, feature engineering, handling of NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[[\"ctrycode_desc\", \"cltdob_fix\", \"min_occ_date\", \"stat_flag\", \"is_housewife_retiree\", \"is_sg_pr\", \"annual_income_est\",\n",
    "             \"hh_size_est\", \"is_consent_to_mail\", \"is_consent_to_email\", \"is_consent_to_call\", \"is_consent_to_sms\", \"flg_substandard\",\n",
    "            \"flg_is_borderline_standard\", \"flg_is_rental_flat\", \"flg_has_health_claim\", \"flg_has_life_claim\", \"flg_gi_claim\",\n",
    "            \"flg_is_proposal\", \"flg_is_returned_mail\",'n_months_last_bought_products', 'flg_latest_being_lapse', 'flg_latest_being_cancel', \n",
    "             'tot_inforce_pols', 'tot_cancel_pols', 'f_ever_declined_la', 'f_elx', 'f_mindef_mha', 'f_retail', 'affcon_visit_days', \n",
    "             'clmcon_visit_days', \"f_purchase_lh\"]].copy()\n",
    "\n",
    "#replacing NaN values\n",
    "new_df.loc[new_df['tot_cancel_pols'].isna(), 'tot_cancel_pols'] = 0\n",
    "new_df.loc[new_df['f_ever_declined_la'].isna(), 'f_ever_declined_la'] = 0\n",
    "new_df.loc[new_df['affcon_visit_days'].isna(), 'affcon_visit_days'] = 0\n",
    "new_df.loc[new_df['clmcon_visit_days'].isna(), 'clmcon_visit_days'] = 0\n",
    "new_df.loc[df[\"f_purchase_lh\"].isna(), \"f_purchase_lh\"] = 0\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Get today's date as a datetime object\n",
    "today_date = datetime.today()\n",
    "\n",
    "#Get the ages of the clients\n",
    "new_df[\"age\"] = (today_date - pd.to_datetime(new_df[\"cltdob_fix\"], errors='coerce')).astype('<m8[Y]') #errors='coerce' will replace invalid parsing or None values with NaT\n",
    "\n",
    "#Get the number of years since the client's first interaction or policy purchase with the company\n",
    "new_df[\"num_years_since_first_interaction\"] = (today_date - pd.to_datetime(new_df[\"min_occ_date\"], errors='coerce')).astype('<m8[Y]')\n",
    "\n",
    "'''\n",
    "count=1\n",
    "for column in new_df.columns:\n",
    "    if new_df[column].isna().sum()==0 and count>=21:\n",
    "        print(f\"na_values in {column}: {new_df[column].isna().sum()}\")\n",
    "        print(f\"value_counts in {column}: {new_df[column].value_counts()}\")\n",
    "    count+=1\n",
    "'''\n",
    "\n",
    "#Replacing certain values with np.nan\n",
    "new_df[\"ctrycode_desc\"].replace(\"Not Applicable\", np.nan, inplace=True)\n",
    "\n",
    "#Replacing the NA values in annual_income_est and hh_size_est columns with the mode as the number of NA values is quite high and removing them will remove quite a lot of rows\n",
    "new_df[\"annual_income_est\"].replace(np.nan, new_df[\"annual_income_est\"].mode().iloc[0], inplace=True)\n",
    "new_df[\"hh_size_est\"].replace(np.nan, new_df[\"hh_size_est\"].mode().iloc[0], inplace=True)\n",
    "\n",
    "#Drop the other NA values\n",
    "new_df.dropna(inplace=True)\n",
    "\n",
    "print(new_df.isna().sum().sum())\n",
    "\n",
    "#Combine is_consent_to_mail, email, call and sms into one column\n",
    "def combine_communications(mail, email, call, sms):\n",
    "    results=[]\n",
    "    for i in range(len(mail)):\n",
    "        if mail.iloc[i]==0 and email.iloc[i]==0 and call.iloc[i]==0 and sms.iloc[i]==0:\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "    return results\n",
    "\n",
    "new_df[\"is_consent_to_communications\"] = combine_communications(new_df[\"is_consent_to_mail\"], new_df[\"is_consent_to_email\"], new_df[\"is_consent_to_call\"], new_df[\"is_consent_to_sms\"])    \n",
    "\n",
    "#Combine flg_has_health_claim, flg_has_life_claim, flg_gi_claim\n",
    "def combine_claims(health, life, gi):\n",
    "    results=[]\n",
    "    for i in range(len(health)):\n",
    "        if health.iloc[i]==0 and life.iloc[i]==0 and gi.iloc[i]==0:\n",
    "            results.append(0)\n",
    "        else:\n",
    "            results.append(1)\n",
    "    return results\n",
    "\n",
    "new_df[\"flg_has_claims\"] = combine_claims(new_df[\"flg_has_health_claim\"], new_df[\"flg_has_life_claim\"], new_df[\"flg_gi_claim\"])\n",
    "\n",
    "new_df.drop([\"cltdob_fix\", \"min_occ_date\", \"is_consent_to_mail\", \"is_consent_to_email\", \"is_consent_to_call\", \"is_consent_to_sms\", \"flg_has_health_claim\", \"flg_has_life_claim\", \"flg_gi_claim\"], axis=1, inplace=True)\n",
    "\n",
    "#Changing the column types \n",
    "target_dtypes = {\"ctrycode_desc\": 'category', \"stat_flag\": 'category', \"is_housewife_retiree\": 'category', \"is_sg_pr\": 'category',\n",
    "                 \"annual_income_est\": 'category', \"hh_size_est\" : 'category', \"flg_substandard\": 'category', \"flg_is_borderline_standard\": 'category', \n",
    "                 \"flg_is_rental_flat\" : 'category', \"flg_is_proposal\" : 'category', \"flg_is_returned_mail\": 'category',\n",
    "                 \"n_months_last_bought_products\" : int, \"flg_latest_being_lapse\" : 'category', \"flg_latest_being_cancel\":'category', \n",
    "                 \"tot_inforce_pols\": int, \"tot_cancel_pols\" : int, \"f_ever_declined_la\" : 'category', \"f_elx\": 'category',\n",
    "                 \"f_mindef_mha\" : 'category', \"f_retail\" : 'category', \"affcon_visit_days\" : int, \"clmcon_visit_days\" : int,\n",
    "                 \"age\": int, \"num_years_since_first_interaction\": int, \"is_consent_to_communications\": 'category',\n",
    "                 \"flg_has_claims\" :'category'}\n",
    "\n",
    "new_df = new_df.apply(lambda x: x.astype(target_dtypes.get(x.name)))\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"is_sg_pr\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"is_consent_to_mail\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"is_consent_to_email\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"is_consent_to_call\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"is_consent_to_sms\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_substandard\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_is_borderline_standard\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_is_rental_flat\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_has_health_claim\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_has_life_claim\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_gi_claim\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_is_proposal\"].isna().sum())\n",
    "print(new_df[new_df[\"is_housewife_retiree\"].isna()][\"flg_is_returned_mail\"].isna().sum())\n",
    "\n",
    "#Whenever the value in the column is_housewife_retiree is nan, the values in the columns is_sg_pr, is_consent_to_mail,\n",
    "#is_consent_to_email, is_consent_to_call, is_consent_to_sms will be nan also. \n",
    "\n",
    "print(new_df[new_df[\"annual_income_est\"].isna()][\"hh_size_est\"].isna().sum())\n",
    "#Whenever the value in the column annual_income_est is nan, the value in the hh_size_est is nan too.\n",
    "\n",
    "'''\n",
    "Thus, the same rows will be removed from the dataset when we remove NA values. \n",
    "This ensures that the number of rows removed from the dataset is not too high\n",
    "so we still retain most information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and split dataset into train test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the target column\n",
    "y = new_df[\"f_purchase_lh\"].copy()\n",
    "\n",
    "#One hot encoding of nominal categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_nominal_cat = new_df.select_dtypes(include='category')\n",
    "X_nominal_cat.drop([\"annual_income_est\", \"hh_size_est\"], axis=1, inplace=True)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "one_hot_df = pd.DataFrame(one_hot_encoder.fit_transform(X_nominal_cat), columns=one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "#Encoding of ordinal categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_ordinal_cat = new_df[[\"annual_income_est\", \"hh_size_est\"]].copy()\n",
    "\n",
    "'''\n",
    "label_encoder = LabelEncoder()\n",
    "for column in X_ordinal_cat.columns:\n",
    "    X_ordinal_cat[column] = label_encoder.fit_transform(X_ordinal_cat[column])\n",
    "print(X_ordinal_cat)\n",
    "'''\n",
    "\n",
    "label_mapping_income = {\"A.ABOVE200K\": 0, \"B.100K-200K\": 1, \"C.60K-100K\": 2, \"D.30K-60K\": 3, \"E.BELOW30K\": 4}\n",
    "X_ordinal_cat[\"annual_income_est\"] = X_ordinal_cat[\"annual_income_est\"].map(label_mapping_income)\n",
    "X_ordinal_cat[\"hh_size_est\"].replace(\">4\", 5, inplace=True)\n",
    "\n",
    "#Scaling of numerical variables\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_numerical = new_df.select_dtypes(include=int)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(X_numerical), columns=X_numerical.columns)\n",
    "\n",
    "#Combine the 3 dataframes together\n",
    "X_preprocessed = pd.concat([one_hot_df, X_ordinal_cat, scaled_df], axis=1, ignore_index=True) #The indexes cannot be different, must reset indexes\n",
    "\n",
    "#Split into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Address imbalance in dataset\n",
    "#Note: Important to do this after splitting the dataset, to prevent leaking information to the test dataset\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    " '''\n",
    " Reason why we chose oversampler over undersampler and SMOTE:\n",
    " 1. Since the target column distribution is 4% 1.0 and 96% 0.0, using undersampler will result in \n",
    " great loss of information, which will impact the model performance significantly.\n",
    " 2. Since there are many categorical variables, we are unable to use SMOTE to increase the number of samples\n",
    " for the under represented class.\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reasons why we chose random forest: \n",
    "It is a good choice for imbalanced data because ensemble nature helps in capturing complex relationships in the data \n",
    "and reduces the risk of overfitting to the majority class. Also, Decision trees, the building blocks of Random Forest, \n",
    "are less sensitive to outliers and noise. This can be beneficial when dealing with imbalanced datasets, \n",
    "as noisy instances or outliers in the majority class may not heavily influence the overall model.\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test) \n",
    "#it's important to use the original, unmodified test data as the purpose of the test set is to \n",
    "#simulate real-world performance, and modifying it with oversampling could lead to overly optimistic performance estimates.\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification (with hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', random_state=42) \n",
    "\n",
    "'''\n",
    "1. class_weight='balanced': automatically adjusts the weights based on the number of samples in each class\n",
    "2. limiting the depth of the trees and increasing the minimum number of samples required to split a node can\n",
    "help prevent overfitting on the majority class\n",
    "3. Increasing  min_samples_leaf helps in preventing the creation of small leaf nodes that might capture noise.\n",
    "4. Limiting the number of features considered for each split can add more randomness and reduce overfitting.\n",
    "5. Increasing the number of trees in the forest can lead to a more stable model (The results will not change that much each time we run the code).\n",
    "'''\n",
    "\n",
    "param_grid = {'n_estimators': [80,100,120], \n",
    "             'max_depth': [10,15,20],\n",
    "             'min_samples_split': [5,10,15],\n",
    "             'min_samples_leaf': [3,5,8],\n",
    "             'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, scoring=['recall_weighted', 'precision_weighted'], refit='precision_weighted')\n",
    "#multi metrics allows you to optimize multiple evaluation metrics simultaneously\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test) \n",
    "# it's important to use the original, unmodified test data as the purpose of the test set is to \n",
    "# simulate real-world performance, and modifying it with oversampling could lead to overly optimistic performance estimates.\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the best parameters from hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42, max_depth = best_params['max_depth'], \n",
    "                                       max_features = best_params['max_features'],\n",
    "                                       min_samples_leaf = best_params['min_samples_leaf'],\n",
    "                                      min_samples_split = best_params['min_samples_split'],\n",
    "                                      n_estimators = best_params['n_estimators'])\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test) \n",
    "#it's important to use the original, unmodified test data as the purpose of the test set is to \n",
    "#simulate real-world performance, and modifying it with oversampling could lead to overly optimistic performance estimates.\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted') #average='weighted' accounts for label imbalance\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"auc_roc: {auc_roc:.2f}\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display confusion matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Get probabilities for the positive class\n",
    "y_probs = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_probs):.2f}')\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Random Forest')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[sorted_indices], align='center')\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot a single decision tree from the Random Forest\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(rf_classifier.estimators_[0], feature_names=X_train.columns, filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    new_df = hidden_data[[\"ctrycode_desc\", \"cltdob_fix\", \"min_occ_date\", \"stat_flag\", \"is_housewife_retiree\", \"is_sg_pr\", \"annual_income_est\",\n",
    "             \"hh_size_est\", \"is_consent_to_mail\", \"is_consent_to_email\", \"is_consent_to_call\", \"is_consent_to_sms\", \"flg_substandard\",\n",
    "            \"flg_is_borderline_standard\", \"flg_is_rental_flat\", \"flg_has_health_claim\", \"flg_has_life_claim\", \"flg_gi_claim\",\n",
    "            \"flg_is_proposal\", \"flg_is_returned_mail\",'n_months_last_bought_products', 'flg_latest_being_lapse', 'flg_latest_being_cancel', \n",
    "             'tot_inforce_pols', 'tot_cancel_pols', 'f_ever_declined_la', 'f_elx', 'f_mindef_mha', 'f_retail', 'affcon_visit_days', \n",
    "             'clmcon_visit_days', \"f_purchase_lh\"]].copy()\n",
    "\n",
    "    #replacing NaN values\n",
    "    new_df.loc[new_df['tot_cancel_pols'].isna(), 'tot_cancel_pols'] = 0\n",
    "    new_df.loc[new_df['f_ever_declined_la'].isna(), 'f_ever_declined_la'] = 0\n",
    "    new_df.loc[new_df['affcon_visit_days'].isna(), 'affcon_visit_days'] = 0\n",
    "    new_df.loc[new_df['clmcon_visit_days'].isna(), 'clmcon_visit_days'] = 0\n",
    "    new_df.loc[new_df[\"f_purchase_lh\"].isna(), \"f_purchase_lh\"] = 0\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Get today's date as a datetime object\n",
    "    today_date = datetime.today()\n",
    "\n",
    "    #Get the ages of the clients\n",
    "    new_df[\"age\"] = (today_date - pd.to_datetime(new_df[\"cltdob_fix\"], errors='coerce')).astype('<m8[Y]') #errors='coerce' will replace invalid parsing or None values with NaT\n",
    "\n",
    "    #Get the number of years since the client's first interaction or policy purchase with the company\n",
    "    new_df[\"num_years_since_first_interaction\"] = (today_date - pd.to_datetime(new_df[\"min_occ_date\"], errors='coerce')).astype('<m8[Y]')\n",
    "\n",
    "    #Replacing certain values with np.nan\n",
    "    new_df[\"ctrycode_desc\"].replace(\"Not Applicable\", np.nan, inplace=True)\n",
    "\n",
    "    #Replacing the NA values in annual_income_est and hh_size_est columns with the mode as the number of NA values is quite high and removing them will remove quite a lot of rows\n",
    "    new_df[\"annual_income_est\"].replace(np.nan, new_df[\"annual_income_est\"].mode().iloc[0], inplace=True)\n",
    "    new_df[\"hh_size_est\"].replace(np.nan, new_df[\"hh_size_est\"].mode().iloc[0], inplace=True)\n",
    "\n",
    "    #Drop the other NA values\n",
    "    new_df.dropna(inplace=True)\n",
    "\n",
    "    #Combine is_consent_to_mail, email, call and sms into one column\n",
    "    def combine_communications(mail, email, call, sms):\n",
    "        results=[]\n",
    "        for i in range(len(mail)):\n",
    "            if mail.iloc[i]==0 and email.iloc[i]==0 and call.iloc[i]==0 and sms.iloc[i]==0:\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(1)\n",
    "        return results\n",
    "\n",
    "    new_df[\"is_consent_to_communications\"] = combine_communications(new_df[\"is_consent_to_mail\"], new_df[\"is_consent_to_email\"], new_df[\"is_consent_to_call\"], new_df[\"is_consent_to_sms\"])    \n",
    "\n",
    "    #Combine flg_has_health_claim, flg_has_life_claim, flg_gi_claim\n",
    "    def combine_claims(health, life, gi):\n",
    "        results=[]\n",
    "        for i in range(len(health)):\n",
    "            if health.iloc[i]==0 and life.iloc[i]==0 and gi.iloc[i]==0:\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(1)\n",
    "        return results\n",
    "\n",
    "    new_df[\"flg_has_claims\"] = combine_claims(new_df[\"flg_has_health_claim\"], new_df[\"flg_has_life_claim\"], new_df[\"flg_gi_claim\"])\n",
    "\n",
    "    new_df.drop([\"cltdob_fix\", \"min_occ_date\", \"is_consent_to_mail\", \"is_consent_to_email\", \"is_consent_to_call\", \"is_consent_to_sms\", \"flg_has_health_claim\", \"flg_has_life_claim\", \"flg_gi_claim\"], axis=1, inplace=True)\n",
    "\n",
    "    #Changing the column types \n",
    "    target_dtypes = {\"ctrycode_desc\": 'category', \"stat_flag\": 'category', \"is_housewife_retiree\": 'category', \"is_sg_pr\": 'category',\n",
    "                     \"annual_income_est\": 'category', \"hh_size_est\" : 'category', \"flg_substandard\": 'category', \"flg_is_borderline_standard\": 'category', \n",
    "                     \"flg_is_rental_flat\" : 'category', \"flg_is_proposal\" : 'category', \"flg_is_returned_mail\": 'category',\n",
    "                     \"n_months_last_bought_products\" : int, \"flg_latest_being_lapse\" : 'category', \"flg_latest_being_cancel\":'category', \n",
    "                     \"tot_inforce_pols\": int, \"tot_cancel_pols\" : int, \"f_ever_declined_la\" : 'category', \"f_elx\": 'category',\n",
    "                     \"f_mindef_mha\" : 'category', \"f_retail\" : 'category', \"affcon_visit_days\" : int, \"clmcon_visit_days\" : int,\n",
    "                     \"age\": int, \"num_years_since_first_interaction\": int, \"is_consent_to_communications\": 'category',\n",
    "                     \"flg_has_claims\" :'category'}\n",
    "\n",
    "    new_df = new_df.apply(lambda x: x.astype(target_dtypes.get(x.name)))\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    \n",
    "    #preparing the target column\n",
    "    y = new_df[\"f_purchase_lh\"].copy()\n",
    "\n",
    "    #One hot encoding of nominal categorical variables\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    X_nominal_cat = new_df.select_dtypes(include='category')\n",
    "    X_nominal_cat.drop([\"annual_income_est\", \"hh_size_est\"], axis=1, inplace=True)\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoder.fit_transform(X_nominal_cat), columns=one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "    #Encoding of ordinal categorical variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    X_ordinal_cat = new_df[[\"annual_income_est\", \"hh_size_est\"]].copy()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in X_ordinal_cat.columns:\n",
    "        X_ordinal_cat[column] = label_encoder.fit_transform(X_ordinal_cat[column])\n",
    "\n",
    "    #Scaling of numerical variables\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    X_numerical = new_df.select_dtypes(include=int)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(X_numerical), columns=X_numerical.columns)\n",
    "\n",
    "    #Combine the 3 dataframes together\n",
    "    X_preprocessed = pd.concat([one_hot_df, X_ordinal_cat, scaled_df], axis=1, ignore_index=True) #The indexes cannot be different, must reset indexes\n",
    "\n",
    "    #Split into train and test datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Address imbalance in dataset\n",
    "    #Note: Important to do this after splitting the dataset, to prevent leaking information to the test dataset\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Create a Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, max_depth = 20, \n",
    "                                           max_features = 'auto',\n",
    "                                           min_samples_leaf = 3,\n",
    "                                          min_samples_split = 10,\n",
    "                                          n_estimators = 120)\n",
    "\n",
    "    # Train the model\n",
    "    rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf_classifier.predict(X_test) \n",
    "    results = list(y_pred) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stacy\\anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Stacy\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "#test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
